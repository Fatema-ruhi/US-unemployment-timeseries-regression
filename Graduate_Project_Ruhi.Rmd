---
output:
  html_document: default
  pdf_document: default
---
A Statistical Analysis of Unemployment Rate In The United States of America Using The Time-Series Method.

Fatema Yesmen Ruhi (Graduate Student)

---



```{r include=FALSE}
setwd("~/Graduate_Project_MA625_F2021")
```

# Abstract
In this project, we have worked with a real data set of 862 unemployment rate from January 1, 1948 to October 1,2021. We applied the mostly the linear regression and time series method to predict the future scenarios of unemployment rate in USA.
We have added two more predictive variables: COVID 19 Declared Public Health Emergency and COVID 19 Declared pandemic to see how the COVID years put an impact on the unemployment. We have selected four important predictors based on the P-values and created a model that fits 98.63% with the dataset. 
Based on the model, our final result shows us that the unemployment rate of future three months are 4.794219, 4.921668 and 5.250166.

This project develops a predictive model for studying the rate of unemployment in the United States for the period. Let $y_t$ be the current unemployment rate which is the response variable and up to lag 24 ($y_{t-1}, \cdots, y_{t-24}$)were used as the predictor variables.
$$y_t = \beta_0 + \beta_1y_{t-1} + \cdots + \beta_{24}y_{t-24} + \alpha_1 x_1 + \alpha_2x_2 + \epsilon$$

# Introduction
This project introduces four statistical methods to evaluate and predict our data: linear regression, Time series, standardizing the predictors and Identifying the ouliers. Linear regression points out which predictors are important and how the model fits with the data set, the time series is applied to find out the systematic pattern overtime and use the pattern to predict the likelihood of future values. Standardizing a predictor means the mean of x (predictor), and the standard deviation of x. We standardize a predictor to make sure all variables contribute evenly to a scale when items are added together, or to make it easier to interpret results of a regression or other analysis. We have also tested the standardized residuals, studentized residuals, leverage points and cook's distance to find out the influential points of the data set.
Our plan is at first take lag 24 to add 24 predictive variables to our original data using the time series method and two other dummy variables related to COVID-19 situation. Hence the our regression equation becomes $$y_t = \beta_0 + \beta_1y_{t-1} + \cdots + \beta_{24}y_{t-24} + \alpha_1 x_1 + \alpha_2x_2 + \epsilon$$, make a model on these variables and notice the important predictors based on the p-value. Then, identify the outliers using the residual methods, remove all the influential points from the data and make a new model to check compare the improvement of our output. We can compare the adjusted R-squared value to understand the goodness of fit of the models. 


# Data Description
Our data is collected from https://fred.stlouisfed.org/series/UNRATE with 862 rows and 2 columns ( date and Unemployment rate). We consider the Unemployment rate as the response variable.. In order to extend the data set without changing the data pattern, we choose lags to create 24 qualitative explanatory variables(xj; j = 1,2,...,24)  and add another two qualitative variables (x25 as COVID 19 Declared Public Health Emergency of International Concern on 30 January 2020; 1 = Public_Health_Emergency and 0 = Not Public_Health_Emergency and x26 as COVID 19 Declared pandemic on 11 March 2020; 1= if the month is under COVID pandemic, 0= if it is not). 
Our equation stands for, $$y_t = \beta_0 + \beta_1y_{t-1} + \cdots + \beta_{24}y_{t-24} + \alpha_1 x_1 + \alpha_2x_2 + \epsilon$$ where, beta and alpha are the coefficients of the variables. 


```{r echo=FALSE}
#setwd("C:/Users/ethom/Downloads")
xt <- read.csv("UNRATE.csv", header = TRUE)
time <- data.frame(number = 1:nrow(xt), date = xt[,1])
```

```{r echo=FALSE}
# COVID 19 Declared Public Health Emergency of International Concern on 30 January 2020
# 1 = Public_Health_Emergency; 0 = Not Public_Health_Emergency
x25 <- ifelse(time$number > 864, 1, 0)
x25 <- x25[25:length(x25)]
```

```{r echo=FALSE}
# COVID 19 Declared pandemic on 11 March 2020
x26 <- ifelse(time$number > 867, 1, 0)
x26 <- x26[25:length(x26)]
```

```{r warning=FALSE, include=FALSE}
xt <- xt[,2]
library(NTS)
frm <- NNsetting(xt,nfore=0,lags=c(1:24))
Predictors <- as.data.frame(frm$X)
colnames(Predictors) <- paste("x", 1:ncol(Predictors), sep = "")
y <- frm$y
data_set<- cbind(y, Predictors, x25, x26)
```


Our 1st model (MOD) is based on the equation mentioned above. The output shows that only x1 is important among all the predictors, though the adjusted R^2 value (93.85%)is significant for the model.

```{r echo=FALSE}
# First Model with 26 predictors and one response
MOD <- lm(y~ ., data = data_set)
summary(MOD)
```

```{r echo=FALSE}
#checking the outliers of model-1
data_set$fitted <- fitted.values(MOD)
data_set$res <- residuals(MOD)
data_set$stad <- rstandard(MOD)
library(MASS)
data_set$stud <- studres(MOD)
data_set$lev <- hatvalues(MOD)
data_set$cooks <- cooks.distance(MOD)
```

Then we test the fiited values, standardized residuals, studentized residuals, leverage points and cook's distance of the model. Using the tested values, we create some plots to represent the points of the model.
We create the plot-1 of `residuals` against `fitted values`. It is shown that, most of the points are gathering to the horizontal line, with a distinct pattern which is an indication of nonlinear relationship.

```{r echo=FALSE}
#plot-1
plot(data_set$fitted, data_set$res, xlab = "Fitted Values", ylab = "Residuals", pch = 19)
abline(h=0)
grid()
```

Plot-2 and Plot-3 are about the `Standardized residuals` against `observations` and `Studentized Residuals` against `observations`respectively. Here, we take the threshold line as 3 and the figures say, some of the points are above the threshold line. we have check further whether these points are influencing our model.

```{r echo=FALSE}
#plot-2
plot(abs(data_set$stad), xlab = "Observation", ylab = "Standardized Residuals", pch = 19)
abline(h = 3, col = "darkred", lwd = 5)
grid()
```

```{r echo=FALSE}
#plot-3
plot(abs(data_set$stud), xlab = "Observation", ylab = "Studentized Residuals", pch = 19)
abline(h = 3, col = "darkred", lwd = 5)
grid()
```


Plot-4 and Plot-5 are about the `leverage` against `observations` and `cook's distance` against `observations`respectively and shows that there are 33 outliers in the data set that are considered as influential points.

```{r echo=FALSE}
#plot-4
plot(data_set$lev, xlab = "Observation", ylab = "Leverage", pch = 19)
k <- 26
threshold <- ((3*k)+ 2)/nrow(data_set)
abline(h = threshold, col = "darkred", lwd = 3)
grid()
```

```{r echo=FALSE}
#plot-5
plot(data_set$cooks, xlab = "Observation", ylab = "Cook's Distance", pch = 19)
threshold <- 3/nrow(data_set)
abline(h = threshold, col = "darkgreen", lwd = 3)
grid()
```


Variance inflation factor (VIF) is a measurement of the amount of multicollinearity in a set of multiple regression variables. It is acceptable when VIF < 10. The VIF of model-1 (MOD) is showing that the multicolinearity is too high among x1 to x24, which is not good for the model. To make a better model, we need to reduce it.

```{r warning=FALSE, include=FALSE}
library(car)
ref_VIF<- vif(MOD)
ref_VIF
```


Now it is time to scale/standardize our predictors (x1,x2,....,x24) and name them as x1_1, x2_1,..., x24_1 respectively. Our new equation is y_t = \beta_0 + \beta_1y_{t-1}_1 + \cdots +\beta_{24}y_{t-24}_1 + \alpha_1 x_1 + \alpha_2x_2 + \epsilon.\end{equation}.
We take an interaction between x1_1 and x25 (x1_1*x25). The presence of an interaction indicates that the effect of one predictor variable on the response variable is different at different values of the other predictor variable.Adding an interaction term to a model drastically changes the interpretation of all the coefficients.We want to see the changes in unemployment rate during the COVID in model-2 (MOD_2, y ~ x1_1 + x2_1 + x25 + x1_1 * x25). From the summary of the model, we find that all the predictors are important by P-values and the adjusted R^2 value is 94.91% which is better than the that of model-1. The confidence intervals do not include 1, that indicates that none of the variables are equal to each other.

```{r echo=FALSE}
#scaling the predictors of data set-1
data_set$x1_1 <- scale(data_set$x1)
data_set$x2_1 <- scale(data_set$x2)
data_set$x3_1 <- scale(data_set$x3)
data_set$x4_1 <- scale(data_set$x4)
data_set$x5_1 <- scale(data_set$x5)
data_set$x6_1 <- scale(data_set$x6)
data_set$x7_1 <- scale(data_set$x7)
data_set$x8_1 <- scale(data_set$x8)
data_set$x9_1 <- scale(data_set$x9)
data_set$x10_1 <- scale(data_set$x10)
data_set$x11_1 <- scale(data_set$x11)
data_set$x12_1 <- scale(data_set$x12)
data_set$x13_1 <- scale(data_set$x13)
data_set$x14_1 <- scale(data_set$x14)
data_set$x15_1 <- scale(data_set$x15)
data_set$x16_1 <- scale(data_set$x16)
data_set$x17_1 <- scale(data_set$x17)
data_set$x18_1 <- scale(data_set$x18)
data_set$x19_1 <- scale(data_set$x19)
data_set$x20_1 <- scale(data_set$x20)
data_set$x21_1 <- scale(data_set$x21)
data_set$x22_1 <- scale(data_set$x22)
data_set$x23_1 <- scale(data_set$x23)
data_set$x24_1 <- scale(data_set$x24)
```

```{r echo=FALSE}
#creating the model-2 based on scaled data set.
MOD_2 <- lm(y~ x1_1 + x2_1 + x25 + x1_1*x25, data = data_set)
summary (MOD_2)
```

```{r echo=FALSE}
#95% confidence interval of model-2
round(cbind(Estimate = coef(MOD_2), confint(MOD_2, level = 0.95)), 4)
```

We again checked the standard residuals in plot-6 to see if there is any outliers. 

```{r echo=FALSE}
#plot-6
#mod_2 res values
data_set$stad1 <- rstandard(MOD_2)
plot(abs(data_set$stad1), xlab = "Observation", ylab = "Standardized Residuals", , pch = 19)
abline(h = 3, col = "darkred", lwd = 5)
grid()
```
Check the variance inflation factor of Model-2.

```{r echo=FALSE}
VIF_2<- vif(MOD_2)
VIF_2
```

Since we have found some outlier points in plot-6, we delete the points that are above 3 by filter function.The number of rows are now 857 i.e., the influencing points has been removed from our data and name the new data by data set-2(data_set_2).

```{r warning=FALSE, include=FALSE}
#deleting the outlier points of model-2
data_set$stad1 <- rstandard(MOD_2)
library(tidyverse)
data_set_2 <- data_set %>%
filter(abs(stad1) <= 3)
nrow(data_set_2)
nrow(data_set)
data_set_2[data_set_2$stad1 > 3, ]
```


Since in data set-2, there are a lot of unnecessary variables, we want to crop only necessary predictors and make a new data set-3. This new data set includes the response variable (y), scaled variable of x1 (x1_1), scaled variable of x2 (x2_1), x25 and x1_1*x25 and name them as 'unemp', 'lag_1', 'lag_2', 'covid' and 'inter' respectively. 
Now we build our final model by data set-3.the summary of Model-3 (unemp ~ lag_1 + lag_2 + covid + inter) shows that it is a better fit than the model-2 by the adjusted R^2 value which is 98.63%. At the same time, our predictor variables are also significant (p-value > .05) in this model.

```{r echo=FALSE}
#without infulential points
data_set_3 <- data.frame(unemp = data_set_2$y, covid = data_set_2$x25, 
              lag_1 = data_set_2$x1_1, lag_2 = data_set_2$x2_1,
              inter = data_set_2$x25*data_set_2$x1_1)
MOD_3 <- lm(unemp ~ lag_1 + lag_2 + covid + inter, data = data_set_3)
summary (MOD_3)
```

We again check the multicollinearity and the confidence interval of model-3. the output of VIF says that the variables 'covid' and 'inter' are linearly independent however, the lag_1 and lag_2 are linearly dependent.  

```{r echo=FALSE}
ref_VIF_3<- vif(MOD_3)
ref_VIF_3
```

For 95% confidence interval, all of the variables are different from each other. Because none of the intervals contain 1.

```{r echo=FALSE}
#95% Confidence interval for model-3
round(cbind(Estimate = coef(MOD_3), confint(MOD_3, level = 0.95)), 4)
```


We are finally ready to predict the future unemployment rate in USA.We will predict the unemployment rate for next 3 months. For this , we need to pick the new values for each variables. when lag_1 = -0.71036837, lag_2 = -0.58895120, covid = 1, inter = -0.71036837,the unemployment is 4.794219; lag_1 = -0.583089922, lag_2 = -0.347861883, covid = 1, inter = -0.583089922, the unemployment is 4.921668; lag_1 = -0.347861883, lag_2 = -0.230247863, covid = 1, inter = -0.347861883, the unemployment is 5.250166.

```{r echo=FALSE}
# New value-1 were scaled values of the response variable for the last two periods
new_x <- data.frame(lag_1 = -0.71036837, lag_2 = -0.58895120, covid = 1, inter = -0.71036837)
prediction <- predict(MOD_3, newdata = new_x, interval = "predict", level = 0.95)
prediction
```

```{r echo=FALSE}
# New values-2 were scaled values of the response variable for the last two periods
new_xx <- data.frame(lag_1 = -0.583089922, lag_2 = -0.347861883, covid = 1, inter = -0.583089922)
prediction <- predict(MOD_3, newdata = new_xx, interval = "predict", level = 0.95)
prediction
```

```{r echo=FALSE}
# New values-3 were scaled values of the response variable for the last two periods
new_xxx <- data.frame(lag_1 = -0.347861883, lag_2 = -0.230247863, covid = 1, inter = -0.347861883)
prediction <- predict(MOD_3, newdata = new_xxx, interval = "predict", level = 0.95)
prediction
```

# Model selection and Interpretation
We select the model-3 as our final model. because first of all, we have built the model with significant and scaled variables of the data set. Secondly, the residuals of model-3 is comparatively lower than the other models. thirdly, adjusted R-squared value is higher than the other models. 

using the model-3, the new data provides the unemployment rates as 4.794219, 4.921668 and 5.250166. These values follow the pattern of existing data which indicates that the values are reliable.

# Reference:
1) data : https://fred.stlouisfed.org/series/UNRATE
2) Lecture notes
#Appendix:
```{r}
#setwd("C:/Users/ethom/Downloads")
xt <- read.csv("UNRATE.csv", header = TRUE)
time <- data.frame(number = 1:nrow(xt), date = xt[,1])
```

```{r}
# COVID 19 Declared Public Health Emergency of International Concern on 30 January 2020
# 1 = Public_Health_Emergency; 0 = Not Public_Health_Emergency
x25 <- ifelse(time$number > 864, 1, 0)
x25 <- x25[25:length(x25)]
```

```{r}
# COVID 19 Declared pandemic on 11 March 2020
x26 <- ifelse(time$number > 867, 1, 0)
x26 <- x26[25:length(x26)]
```

```{r warning=FALSE, include=FALSE}
xt <- xt[,2]
library(NTS)
frm <- NNsetting(xt,nfore=0,lags=c(1:24))
Predictors <- as.data.frame(frm$X)
colnames(Predictors) <- paste("x", 1:ncol(Predictors), sep = "")
y <- frm$y
data_set<- cbind(y, Predictors, x25, x26)
```

```{r}
# First Model with 26 predictors and one response
MOD <- lm(y~ ., data = data_set)
summary(MOD)
```

```{r}
#checking the outliers of model-1
data_set$fitted <- fitted.values(MOD)
data_set$res <- residuals(MOD)
data_set$stad <- rstandard(MOD)
library(MASS)
data_set$stud <- studres(MOD)
data_set$lev <- hatvalues(MOD)
data_set$cooks <- cooks.distance(MOD)
```

```{r}
#plot-1
plot(data_set$fitted, data_set$res, xlab = "Fitted Values", ylab = "Residuals", pch = 19)
abline(h=0)
grid()
```

```{r}
#plot-2
plot(abs(data_set$stad), xlab = "Observation", ylab = "Standardized Residuals", pch = 19)
abline(h = 3, col = "darkred", lwd = 5)
grid()
```

```{r}
#plot-3
plot(abs(data_set$stud), xlab = "Observation", ylab = "Studentized Residuals", pch = 19)
abline(h = 3, col = "darkred", lwd = 5)
grid()
```

```{r}
#plot-4
plot(data_set$lev, xlab = "Observation", ylab = "Leverage", pch = 19)
k <- 26
threshold <- ((3*k)+ 2)/nrow(data_set)
abline(h = threshold, col = "darkred", lwd = 3)
grid()
```

```{r}
#plot-4
plot(data_set$lev, xlab = "Observation", ylab = "Leverage", pch = 19)
k <- 26
threshold <- ((3*k)+ 2)/nrow(data_set)
abline(h = threshold, col = "darkred", lwd = 3)
grid()
```

```{r}
#plot-5
plot(data_set$cooks, xlab = "Observation", ylab = "Cook's Distance", pch = 19)
threshold <- 3/nrow(data_set)
abline(h = threshold, col = "darkgreen", lwd = 3)
grid()
```

```{r warning=FALSE, include=FALSE}
library(car)
ref_VIF<- vif(MOD)
ref_VIF
```

```{r}
#scaling the predictors of data set-1
data_set$x1_1 <- scale(data_set$x1)
data_set$x2_1 <- scale(data_set$x2)
data_set$x3_1 <- scale(data_set$x3)
data_set$x4_1 <- scale(data_set$x4)
data_set$x5_1 <- scale(data_set$x5)
data_set$x6_1 <- scale(data_set$x6)
data_set$x7_1 <- scale(data_set$x7)
data_set$x8_1 <- scale(data_set$x8)
data_set$x9_1 <- scale(data_set$x9)
data_set$x10_1 <- scale(data_set$x10)
data_set$x11_1 <- scale(data_set$x11)
data_set$x12_1 <- scale(data_set$x12)
data_set$x13_1 <- scale(data_set$x13)
data_set$x14_1 <- scale(data_set$x14)
data_set$x15_1 <- scale(data_set$x15)
data_set$x16_1 <- scale(data_set$x16)
data_set$x17_1 <- scale(data_set$x17)
data_set$x18_1 <- scale(data_set$x18)
data_set$x19_1 <- scale(data_set$x19)
data_set$x20_1 <- scale(data_set$x20)
data_set$x21_1 <- scale(data_set$x21)
data_set$x22_1 <- scale(data_set$x22)
data_set$x23_1 <- scale(data_set$x23)
data_set$x24_1 <- scale(data_set$x24)
```

```{r}
#creating the model-2 based on scaled data set.
MOD_2 <- lm(y~ x1_1 + x2_1 + x25 + x1_1*x25, data = data_set)
summary (MOD_2)
```

```{r}
#95% confidence interval of model-2
round(cbind(Estimate = coef(MOD_2), confint(MOD_2, level = 0.95)), 4)
```

```{r}
#plot-6
#model-2 residual values
data_set$stad1 <- rstandard(MOD_2)
plot(abs(data_set$stad1), xlab = "Observation", ylab = "Standardized Residuals", , pch = 19)
abline(h = 3, col = "darkred", lwd = 5)
grid()
```

```{r}
VIF_2<- vif(MOD_2)
VIF_2
```

```{r warning=FALSE, include=FALSE}
#deleting the outlier points of model-2
data_set$stad1 <- rstandard(MOD_2)
library(tidyverse)
data_set_2 <- data_set %>%
filter(abs(stad1) <= 3)
nrow(data_set_2)
nrow(data_set)
data_set_2[data_set_2$stad1 > 3, ]
```


```{r}
#creating model-3 without influential points.
data_set_3 <- data.frame(unemp = data_set_2$y, covid = data_set_2$x25, 
              lag_1 = data_set_2$x1_1, lag_2 = data_set_2$x2_1,
              inter = data_set_2$x25*data_set_2$x1_1)
MOD_3 <- lm(unemp ~ lag_1 + lag_2 + covid + inter, data = data_set_3)
summary (MOD_3)
```

```{r}
#checking the multicollinearity of model-3
ref_VIF_3<- vif(MOD_3)
ref_VIF_3
```

```{r}
#95% Confidence interval for model-3
round(cbind(Estimate = coef(MOD_3), confint(MOD_3, level = 0.95)), 4)
```

```{r}
# New value-1 were scaled values of the response variable for the last two periods
new_x <- data.frame(lag_1 = -0.71036837, lag_2 = -0.58895120, covid = 1, inter = -0.71036837)
prediction <- predict(MOD_3, newdata = new_x, interval = "predict", level = 0.95)
prediction

# New values-2 were scaled values of the response variable for the last two periods
new_xx <- data.frame(lag_1 = -0.583089922, lag_2 = -0.347861883, covid = 1, inter = -0.583089922)
prediction <- predict(MOD_3, newdata = new_xx, interval = "predict", level = 0.95)
prediction

# New values-3 were scaled values of the response variable for the last two periods
new_xxx <- data.frame(lag_1 = -0.347861883, lag_2 = -0.230247863, covid = 1, inter = -0.347861883)
prediction <- predict(MOD_3, newdata = new_xxx, interval = "predict", level = 0.95)
prediction
```


